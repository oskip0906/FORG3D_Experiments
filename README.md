## Description

This repository showcases how to use images and scenes generated by the **[FORG3D Rendering Tool](https://github.com/compling-wat/FORG3D)** for training and evaluation.

#### 🛠️ Step 1: Generate Training Data
- Start with an `output/` folder containing the `images/` and `scenes/` directories (as produced by the FORG3D tool).
- This step will:
  - Flatten subdirectories inside `images/` and `scenes/` into two new directories.
  - Generate a list of questions for each scene and store them in a `.csv` file.
  - Convert these into a `.jsonl` file ready for training.

#### 🧠 Step 2: Fine-tune a Vision-Language Model (LoRA)
- Use the provided `example_train.sh` script to perform LoRA fine-tuning with your chosen model and training parameters.
- **Requirements:**
  - A GPU with **CUDA** and **BF16** support.
- Training progress will be logged to **[Wandb](https://wandb.ai/site/)**.
- The fine-tuned model and latest checkpoint will be saved to the specified output directory.

#### 📊 Step 3: Evaluate the Fine-tuned Model
- Run `example_eval.sh` to evaluate the model:
  - On random samples from your dataset.
  - Or using a curated question set from [3DSRBench](https://huggingface.co/papers/2412.07825).
- The script will display correctness per entry and an overall accuracy at the end.

> ✅ **Tip:** It is highly recommended to use a `slurm` script for managing fine-tuning and evaluation jobs on compute clusters.
